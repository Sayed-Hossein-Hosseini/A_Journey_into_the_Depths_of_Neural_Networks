{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+F+OXSIw9D7Df8KowR/KL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sayed-Hossein-Hosseini/A_Journey_into_the_Depths_of_Neural_Networks/blob/master/Union_of_Neurons_in_a_Hidden_Layer_in_a_Binary_Style.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Union of Neurons in a Hidden Layer in a Binary Style**"
      ],
      "metadata": {
        "id": "OmrZS2-4Ryg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Libraries**"
      ],
      "metadata": {
        "id": "Dj3EqH2_RueG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-DR5mD_ALKkT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
        "from a_neuron_dancing_in_logistic_regression_style import NeuralNetwork, DenseLayer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading Dataset and Normalization**"
      ],
      "metadata": {
        "id": "OYs6uj77V1W7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load in the data\n",
        "    cifar10 = tf.keras.datasets.cifar10\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "    X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "    print(X_train.shape)\n",
        "    print(X_test.shape)\n",
        "\n",
        "    print(y_train.shape)\n",
        "    print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58mFWpM0V7Ii",
        "outputId": "edf5753c-da59-4d39-a715-492c3a1e8481"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "(50000, 1)\n",
            "(10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Relabeling Data**"
      ],
      "metadata": {
        "id": "LptW9MkwWAnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Label 0 is for airplane\n",
        "    y_train = np.where(y_train == 0, 0, 1)\n",
        "    y_test = np.where(y_test == 0, 0, 1)\n",
        "\n",
        "    print(\"y_train:\")\n",
        "    print(y_train[160:170])\n",
        "\n",
        "    print(\"y_test:\")\n",
        "    print(y_test[160:170])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TraSF-9tWCYB",
        "outputId": "ff03c8c4-e5a2-4a0d-af9d-7474abda94ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_train:\n",
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "y_test:\n",
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Flatten and Reshape Images**"
      ],
      "metadata": {
        "id": "iDCgNtSdWJlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # reshape images from (32, 32, 3) to (3072,)\n",
        "    X_train = X_train.reshape((X_train.shape[0], -1))  # (50000, 3072)\n",
        "    X_test = X_test.reshape((X_test.shape[0], -1))     # (10000, 3072)\n",
        "\n",
        "    y_train, y_test = y_train.flatten(), y_test.flatten() # (50000,  -   10000,)"
      ],
      "metadata": {
        "id": "TlFXunAzWLno"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Main**\n"
      ],
      "metadata": {
        "id": "2tYz-nu5ZOdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    model = NeuralNetwork()\n",
        "    model.add(DenseLayer(3072, 64, activation='sigmoid'))\n",
        "    model.add(DenseLayer(64, 1, activation='sigmoid'))\n",
        "\n",
        "    model.train(X_train, y_train, epochs=1000, lr=0.01, batch_size=64)\n",
        "    predictions = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXnULG3SZTZu",
        "outputId": "05c5499e-7f25-44b5-a2bf-04d5d2634bdd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 0.3605\n",
            "Epoch 10: Loss = 0.3070\n",
            "Epoch 20: Loss = 0.2901\n",
            "Epoch 30: Loss = 0.2811\n",
            "Epoch 40: Loss = 0.2750\n",
            "Epoch 50: Loss = 0.2716\n",
            "Epoch 60: Loss = 0.2687\n",
            "Epoch 70: Loss = 0.2669\n",
            "Epoch 80: Loss = 0.2660\n",
            "Epoch 90: Loss = 0.2655\n",
            "Epoch 100: Loss = 0.2650\n",
            "Epoch 110: Loss = 0.2649\n",
            "Epoch 120: Loss = 0.2649\n",
            "Epoch 130: Loss = 0.2649\n",
            "Epoch 140: Loss = 0.2649\n",
            "Epoch 150: Loss = 0.2647\n",
            "Epoch 160: Loss = 0.2647\n",
            "Epoch 170: Loss = 0.2643\n",
            "Epoch 180: Loss = 0.2642\n",
            "Epoch 190: Loss = 0.2637\n",
            "Epoch 200: Loss = 0.2634\n",
            "Epoch 210: Loss = 0.2628\n",
            "Epoch 220: Loss = 0.2626\n",
            "Epoch 230: Loss = 0.2622\n",
            "Epoch 240: Loss = 0.2612\n",
            "Epoch 250: Loss = 0.2610\n",
            "Epoch 260: Loss = 0.2603\n",
            "Epoch 270: Loss = 0.2597\n",
            "Epoch 280: Loss = 0.2592\n",
            "Epoch 290: Loss = 0.2587\n",
            "Epoch 300: Loss = 0.2583\n",
            "Epoch 310: Loss = 0.2577\n",
            "Epoch 320: Loss = 0.2569\n",
            "Epoch 330: Loss = 0.2565\n",
            "Epoch 340: Loss = 0.2561\n",
            "Epoch 350: Loss = 0.2558\n",
            "Epoch 360: Loss = 0.2555\n",
            "Epoch 370: Loss = 0.2551\n",
            "Epoch 380: Loss = 0.2547\n",
            "Epoch 390: Loss = 0.2541\n",
            "Epoch 400: Loss = 0.2536\n",
            "Epoch 410: Loss = 0.2536\n",
            "Epoch 420: Loss = 0.2529\n",
            "Epoch 430: Loss = 0.2525\n",
            "Epoch 440: Loss = 0.2521\n",
            "Epoch 450: Loss = 0.2518\n",
            "Epoch 460: Loss = 0.2514\n",
            "Epoch 470: Loss = 0.2509\n",
            "Epoch 480: Loss = 0.2508\n",
            "Epoch 490: Loss = 0.2505\n",
            "Epoch 500: Loss = 0.2501\n",
            "Epoch 510: Loss = 0.2500\n",
            "Epoch 520: Loss = 0.2494\n",
            "Epoch 530: Loss = 0.2494\n",
            "Epoch 540: Loss = 0.2494\n",
            "Epoch 550: Loss = 0.2489\n",
            "Epoch 560: Loss = 0.2483\n",
            "Epoch 570: Loss = 0.2481\n",
            "Epoch 580: Loss = 0.2477\n",
            "Epoch 590: Loss = 0.2476\n",
            "Epoch 600: Loss = 0.2473\n",
            "Epoch 610: Loss = 0.2469\n",
            "Epoch 620: Loss = 0.2467\n",
            "Epoch 630: Loss = 0.2464\n",
            "Epoch 640: Loss = 0.2463\n",
            "Epoch 650: Loss = 0.2460\n",
            "Epoch 660: Loss = 0.2457\n",
            "Epoch 670: Loss = 0.2454\n",
            "Epoch 680: Loss = 0.2456\n",
            "Epoch 690: Loss = 0.2450\n",
            "Epoch 700: Loss = 0.2447\n",
            "Epoch 710: Loss = 0.2445\n",
            "Epoch 720: Loss = 0.2442\n",
            "Epoch 730: Loss = 0.2442\n",
            "Epoch 740: Loss = 0.2446\n",
            "Epoch 750: Loss = 0.2435\n",
            "Epoch 760: Loss = 0.2432\n",
            "Epoch 770: Loss = 0.2431\n",
            "Epoch 780: Loss = 0.2428\n",
            "Epoch 790: Loss = 0.2425\n",
            "Epoch 800: Loss = 0.2423\n",
            "Epoch 810: Loss = 0.2420\n",
            "Epoch 820: Loss = 0.2418\n",
            "Epoch 830: Loss = 0.2418\n",
            "Epoch 840: Loss = 0.2413\n",
            "Epoch 850: Loss = 0.2410\n",
            "Epoch 860: Loss = 0.2407\n",
            "Epoch 870: Loss = 0.2403\n",
            "Epoch 880: Loss = 0.2400\n",
            "Epoch 890: Loss = 0.2399\n",
            "Epoch 900: Loss = 0.2396\n",
            "Epoch 910: Loss = 0.2398\n",
            "Epoch 920: Loss = 0.2389\n",
            "Epoch 930: Loss = 0.2388\n",
            "Epoch 940: Loss = 0.2383\n",
            "Epoch 950: Loss = 0.2382\n",
            "Epoch 960: Loss = 0.2377\n",
            "Epoch 970: Loss = 0.2374\n",
            "Epoch 980: Loss = 0.2373\n",
            "Epoch 990: Loss = 0.2369\n"
          ]
        }
      ]
    }
  ]
}