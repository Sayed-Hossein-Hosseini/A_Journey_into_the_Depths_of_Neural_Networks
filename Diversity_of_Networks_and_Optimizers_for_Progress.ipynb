{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjHbR+wCgJx1w6uzAAwd4n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sayed-Hossein-Hosseini/A_Journey_into_the_Depths_of_Neural_Networks/blob/master/Diversity_of_Networks_and_Optimizers_for_Progress.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Diversity of Networks and Optimizers for Progress**"
      ],
      "metadata": {
        "id": "JkYfqlmvS9VE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Libraries**"
      ],
      "metadata": {
        "id": "-VE8nF4oTEKr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d-kXszUQRdF1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading Dataset and Normalization**"
      ],
      "metadata": {
        "id": "iHPDP4x4TIVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load in the data\n",
        "    cifar10 = tf.keras.datasets.cifar10\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "    X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "    print(X_train.shape)\n",
        "    print(X_test.shape)\n",
        "\n",
        "    print(y_train.shape)\n",
        "    print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Meu9eVbTNcO",
        "outputId": "446a124d-b0d7-4e1b-8b55-cd8dfe2d31b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "(50000, 1)\n",
            "(10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Flatten and Reshape Images**"
      ],
      "metadata": {
        "id": "rJV2N5cjTQTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # reshape images from (32, 32, 3) to (3072,)\n",
        "    X_train = X_train.reshape((X_train.shape[0], -1))  # (50000, 3072)\n",
        "    X_test = X_test.reshape((X_test.shape[0], -1))     # (10000, 3072)\n",
        "\n",
        "    y_train, y_test = y_train.flatten(), y_test.flatten() # (50000,  -   10000,)\n",
        "    print(y_train[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYcnZemZTXmu",
        "outputId": "e7e878d7-f927-4562-8648-dfaebdc816b5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6 9 9 4 1 1 2 7 8 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **One-Hot Encoding**"
      ],
      "metadata": {
        "id": "rbKtTb2pTaI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "# y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "print(y_train.shape)\n",
        "# print(y_test.shape)\n",
        "\n",
        "print(y_train[:10])\n",
        "# print(y_test[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IomBB5sKTbCZ",
        "outputId": "99da3154-4cf8-4abd-a38e-20fbce5f6598"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 10)\n",
            "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model**"
      ],
      "metadata": {
        "id": "KoM4zOFDT2Dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Activation Function : Sigmoid and Relu and Softmax and Tanh**"
      ],
      "metadata": {
        "id": "_EGtVE0ST7Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation:\n",
        "    @staticmethod\n",
        "    def sigmoid(z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(a):\n",
        "        return a * (1 - a)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu_derivative(z):\n",
        "        return (z > 0).astype(float)\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(z):\n",
        "        exps = np.exp(z - np.max(z, axis=1, keepdims=True))  # For numerical stability\n",
        "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax_derivative(output):\n",
        "        # The exact derivative of softmax is a Jacobian matrix.\n",
        "        # However, when used with cross-entropy loss, the gradient simplifies to:\n",
        "        # ∂L/∂z = y_pred - y_true\n",
        "        # So this method is typically not needed explicitly during backpropagation.\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(z):\n",
        "        return np.tanh(z)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh_derivative(a):\n",
        "        return 1 - np.power(a, 2)"
      ],
      "metadata": {
        "id": "k3wKPi9CUFes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Neural Network**"
      ],
      "metadata": {
        "id": "v7PuBA2NUpMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dence Layer Class**"
      ],
      "metadata": {
        "id": "M05WPk8HUvIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer:\n",
        "    def __init__(self, input_size, output_size, activation='sigmoid'):\n",
        "        self.w = np.random.randn(input_size, output_size) * 0.01\n",
        "        self.b = np.zeros((1, output_size))\n",
        "        self.activation_name = activation\n",
        "        self.z = None\n",
        "        self.a = None\n",
        "        self.input = None\n",
        "        self.dw = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.input = x\n",
        "        self.z = np.dot(x, self.w) + self.b\n",
        "        if self.activation_name == 'sigmoid':\n",
        "            self.a = Activation.sigmoid(self.z)\n",
        "        elif self.activation_name == 'relu':\n",
        "            self.a = Activation.relu(self.z)\n",
        "        elif self.activation_name == 'tanh':\n",
        "            self.a = Activation.tanh(self.z)\n",
        "        elif self.activation_name == 'softmax':\n",
        "            self.a = Activation.softmax(self.z)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation function: {self.activation_name}\")\n",
        "        return self.a\n",
        "\n",
        "    def backward(self, da):\n",
        "        m = self.input.shape[0]\n",
        "        if self.activation_name == 'sigmoid':\n",
        "            dz = da * Activation.sigmoid_derivative(self.a)\n",
        "        elif self.activation_name == 'relu':\n",
        "            dz = da * Activation.relu_derivative(self.z)\n",
        "        elif self.activation_name == 'tanh':\n",
        "            dz = da * Activation.tanh_derivative(self.a)\n",
        "        elif self.activation_name == 'softmax':\n",
        "            dz = da  # softmax + cross-entropy\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation function: {self.activation_name}\")\n",
        "\n",
        "        self.dw = np.dot(self.input.T, dz) / m\n",
        "        self.db = np.sum(dz, axis=0, keepdims=True) / m\n",
        "        return np.dot(dz, self.w.T)"
      ],
      "metadata": {
        "id": "H7RtNdIhUtN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Optimizer Classes**"
      ],
      "metadata": {
        "id": "N3m_hbAcYAU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "        layer.w -= self.lr * layer.dw\n",
        "        layer.b -= self.lr * layer.db\n",
        "\n",
        "\n",
        "class Momentum:\n",
        "    def __init__(self, lr=0.01, beta=0.9):\n",
        "        self.lr = lr\n",
        "        self.beta = beta\n",
        "        self.v_w = {}\n",
        "        self.v_b = {}\n",
        "\n",
        "    def update(self, layer):\n",
        "        if layer not in self.v_w:\n",
        "            self.v_w[layer] = np.zeros_like(layer.w)\n",
        "            self.v_b[layer] = np.zeros_like(layer.b)\n",
        "        self.v_w[layer] = self.beta * self.v_w[layer] + (1 - self.beta) * layer.dw\n",
        "        self.v_b[layer] = self.beta * self.v_b[layer] + (1 - self.beta) * layer.db\n",
        "        layer.w -= self.lr * self.v_w[layer]\n",
        "        layer.b -= self.lr * self.v_b[layer]\n",
        "\n",
        "\n",
        "class Adam:\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m_w, self.v_w = {}, {}\n",
        "        self.m_b, self.v_b = {}, {}\n",
        "        self.t = {}\n",
        "\n",
        "    def update(self, layer):\n",
        "        if layer not in self.m_w:\n",
        "            self.m_w[layer] = np.zeros_like(layer.w)\n",
        "            self.v_w[layer] = np.zeros_like(layer.w)\n",
        "            self.m_b[layer] = np.zeros_like(layer.b)\n",
        "            self.v_b[layer] = np.zeros_like(layer.b)\n",
        "            self.t[layer] = 0\n",
        "\n",
        "        self.t[layer] += 1\n",
        "\n",
        "        self.m_w[layer] = self.beta1 * self.m_w[layer] + (1 - self.beta1) * layer.dw\n",
        "        self.v_w[layer] = self.beta2 * self.v_w[layer] + (1 - self.beta2) * (layer.dw ** 2)\n",
        "        self.m_b[layer] = self.beta1 * self.m_b[layer] + (1 - self.beta1) * layer.db\n",
        "        self.v_b[layer] = self.beta2 * self.v_b[layer] + (1 - self.beta2) * (layer.db ** 2)\n",
        "\n",
        "        m_w_hat = self.m_w[layer] / (1 - self.beta1 ** self.t[layer])\n",
        "        v_w_hat = self.v_w[layer] / (1 - self.beta2 ** self.t[layer])\n",
        "        m_b_hat = self.m_b[layer] / (1 - self.beta1 ** self.t[layer])\n",
        "        v_b_hat = self.v_b[layer] / (1 - self.beta2 ** self.t[layer])\n",
        "\n",
        "        layer.w -= self.lr * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
        "        layer.b -= self.lr * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)"
      ],
      "metadata": {
        "id": "opbKY-qmYFIg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}