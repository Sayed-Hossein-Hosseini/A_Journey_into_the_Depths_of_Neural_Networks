{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM04hOOZg75k2sAMQ/Hmj+K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sayed-Hossein-Hosseini/A_Journey_into_the_Depths_of_Neural_Networks/blob/master/Entering_the_World_of_Multi_Class_Classifiers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Entering the World of Multi Class Classifiers**"
      ],
      "metadata": {
        "id": "hWHn0Eqs-CnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Libraries**"
      ],
      "metadata": {
        "id": "KguaGt3_-L5O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CAnjJNG52KfX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading Dataset and Normalization**"
      ],
      "metadata": {
        "id": "5E4qHRR1-jET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load in the data\n",
        "    cifar10 = tf.keras.datasets.cifar10\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "    X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "    print(X_train.shape)\n",
        "    print(X_test.shape)\n",
        "\n",
        "    print(y_train.shape)\n",
        "    print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUYvEIMG-jVy",
        "outputId": "2ef3bfd0-91e1-4b49-9a8c-ffdc5b24524c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "(50000, 1)\n",
            "(10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Flatten and Reshape Images**"
      ],
      "metadata": {
        "id": "Jr8zn-yv-wj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # reshape images from (32, 32, 3) to (3072,)\n",
        "    X_train = X_train.reshape((X_train.shape[0], -1))  # (50000, 3072)\n",
        "    X_test = X_test.reshape((X_test.shape[0], -1))     # (10000, 3072)\n",
        "\n",
        "    y_train, y_test = y_train.flatten(), y_test.flatten() # (50000,  -   10000,)\n",
        "    print(y_train[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3ZYECpH-11B",
        "outputId": "22c5b655-0db1-41c0-ec59-2b34f002a247"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6 9 9 4 1 1 2 7 8 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **One-Hot Encoding**"
      ],
      "metadata": {
        "id": "jR-E_GUrBf1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "print(y_train[:10])\n",
        "print(y_test[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n76t9Op8Bhay",
        "outputId": "79f01ad3-33be-4046-b8ec-64d5fff9eb39"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 10)\n",
            "(10000, 10)\n",
            "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model**"
      ],
      "metadata": {
        "id": "_3vU69YNNEjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Activation Function : Sigmoid and Relu and Softmax**"
      ],
      "metadata": {
        "id": "PqAkZHU2NIEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation:\n",
        "    @staticmethod\n",
        "    def sigmoid(z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(a):\n",
        "        return a * (1 - a)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu_derivative(z):\n",
        "        return (z > 0).astype(float)\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(z):\n",
        "        exps = np.exp(z - np.max(z, axis=1, keepdims=True))  # For numerical stability\n",
        "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax_derivative(output):\n",
        "        # The exact derivative of softmax is a Jacobian matrix.\n",
        "        # However, when used with cross-entropy loss, the gradient simplifies to:\n",
        "        # ∂L/∂z = y_pred - y_true\n",
        "        # So this method is typically not needed explicitly during backpropagation.\n",
        "        pass"
      ],
      "metadata": {
        "id": "eYOELm9CNRKb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Neural Network**"
      ],
      "metadata": {
        "id": "F0ig8k7MQj6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dence Layer Class**"
      ],
      "metadata": {
        "id": "yP8UkBT_QnIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer:\n",
        "    def __init__(self, input_size, output_size, activation='sigmoid'):\n",
        "        self.w = np.random.randn(input_size, output_size) * 0.01\n",
        "        self.b = np.zeros((1, output_size))\n",
        "        self.activation_name = activation\n",
        "        self.z = None\n",
        "        self.a = None\n",
        "        self.input = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.input = x\n",
        "        self.z = np.dot(x, self.w) + self.b\n",
        "        if self.activation_name == 'sigmoid':\n",
        "            self.a = Activation.sigmoid(self.z)\n",
        "        elif self.activation_name == 'relu':\n",
        "            self.a = Activation.relu(self.z)\n",
        "        elif self.activation_name == 'softmax':\n",
        "            self.a = Activation.softmax(self.z)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation function: {self.activation_name}\")\n",
        "        return self.a\n",
        "\n",
        "    def backward(self, da, lr):\n",
        "        m = self.input.shape[0]\n",
        "\n",
        "        # Derivative depending on activation function\n",
        "        if self.activation_name == 'sigmoid':\n",
        "            dz = da * Activation.sigmoid_derivative(self.a)\n",
        "        elif self.activation_name == 'relu':\n",
        "            dz = da * Activation.relu_derivative(self.z)\n",
        "        elif self.activation_name == 'softmax':\n",
        "            # With softmax + cross-entropy, da is already (y_pred - y_true)\n",
        "            dz = da\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation function: {self.activation_name}\")\n",
        "\n",
        "        # Compute gradients\n",
        "        dw = np.dot(self.input.T, dz) / m\n",
        "        db = np.sum(dz, axis=0, keepdims=True) / m\n",
        "        da_prev = np.dot(dz, self.w.T)\n",
        "\n",
        "        # Update weights\n",
        "        self.w -= lr * dw\n",
        "        self.b -= lr * db\n",
        "\n",
        "        return da_prev"
      ],
      "metadata": {
        "id": "MK5kgqoXQn_l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Neural Network Class**"
      ],
      "metadata": {
        "id": "uUI5mb0kSyx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        epsilon = 1e-8\n",
        "        if y_true.shape[1] == 1:  # Binary classification\n",
        "            return -np.mean(y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))\n",
        "        else:  # Multi-class classification\n",
        "            return -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=1))\n",
        "\n",
        "    def backward(self, y_true, y_pred, lr):\n",
        "        if y_true.shape[1] == 1:  # Binary classification (sigmoid + BCE)\n",
        "            dz = y_pred - y_true\n",
        "        else:  # Multi-class classification (softmax + CCE)\n",
        "            dz = y_pred - y_true  # Gradient for softmax + cross-entropy\n",
        "        gradient = dz\n",
        "\n",
        "        for layer in reversed(self.layers):\n",
        "            gradient = layer.backward(gradient, lr)\n",
        "\n",
        "    def train(self, X, y, epochs=100, lr=0.01, batch_size=64, verbose=True):\n",
        "        for epoch in range(epochs):\n",
        "            # Shuffle data\n",
        "            indices = np.arange(X.shape[0])\n",
        "            np.random.shuffle(indices)\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            epoch_loss = []\n",
        "\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "                x_batch = X_shuffled[i:i+batch_size]\n",
        "                y_batch = y_shuffled[i:i+batch_size]\n",
        "\n",
        "                # Forward pass\n",
        "                output = x_batch\n",
        "                for layer in self.layers:\n",
        "                    output = layer.forward(output)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.compute_loss(y_batch, output)\n",
        "                epoch_loss.append(loss)\n",
        "\n",
        "                # Backward pass\n",
        "                self.backward(y_batch, output, lr)\n",
        "\n",
        "            if verbose and epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}: Loss = {np.mean(epoch_loss):.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = self.forward(X)\n",
        "        if y_pred.shape[1] == 1:  # Binary classification\n",
        "            return (y_pred >= 0.5).astype(int)\n",
        "        else:  # Multi-class classification\n",
        "            return np.argmax(y_pred, axis=1)"
      ],
      "metadata": {
        "id": "m5TPuv8cS3hh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Main**"
      ],
      "metadata": {
        "id": "LHgjgvPwTX2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    model = NeuralNetwork()\n",
        "    model.add(DenseLayer(3072, 64, activation='sigmoid'))\n",
        "    model.add(DenseLayer(64, 10, activation='softmax'))\n",
        "\n",
        "    model.train(X_train, y_train, epochs=1000, lr=0.05, batch_size=64)\n",
        "    predictions = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvv49eYUTjMX",
        "outputId": "72e301a9-3526-40b3-8652-c5550265ad98"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 2.1652\n",
            "Epoch 10: Loss = 1.6128\n",
            "Epoch 20: Loss = 1.4759\n",
            "Epoch 30: Loss = 1.3954\n",
            "Epoch 40: Loss = 1.3365\n",
            "Epoch 50: Loss = 1.2893\n",
            "Epoch 60: Loss = 1.2487\n",
            "Epoch 70: Loss = 1.2155\n",
            "Epoch 80: Loss = 1.1804\n",
            "Epoch 90: Loss = 1.1551\n",
            "Epoch 100: Loss = 1.1302\n",
            "Epoch 110: Loss = 1.1067\n",
            "Epoch 120: Loss = 1.0837\n",
            "Epoch 130: Loss = 1.0647\n",
            "Epoch 140: Loss = 1.0435\n",
            "Epoch 150: Loss = 1.0283\n",
            "Epoch 160: Loss = 1.0102\n",
            "Epoch 170: Loss = 0.9956\n",
            "Epoch 180: Loss = 0.9789\n",
            "Epoch 190: Loss = 0.9637\n",
            "Epoch 200: Loss = 0.9498\n",
            "Epoch 210: Loss = 0.9339\n",
            "Epoch 220: Loss = 0.9217\n",
            "Epoch 230: Loss = 0.9096\n",
            "Epoch 240: Loss = 0.8977\n",
            "Epoch 250: Loss = 0.8866\n",
            "Epoch 260: Loss = 0.8733\n",
            "Epoch 270: Loss = 0.8608\n",
            "Epoch 280: Loss = 0.8512\n",
            "Epoch 290: Loss = 0.8401\n",
            "Epoch 300: Loss = 0.8300\n",
            "Epoch 310: Loss = 0.8177\n",
            "Epoch 320: Loss = 0.8080\n",
            "Epoch 330: Loss = 0.7993\n",
            "Epoch 340: Loss = 0.7880\n",
            "Epoch 350: Loss = 0.7797\n",
            "Epoch 360: Loss = 0.7741\n",
            "Epoch 370: Loss = 0.7645\n",
            "Epoch 380: Loss = 0.7557\n",
            "Epoch 390: Loss = 0.7469\n",
            "Epoch 400: Loss = 0.7346\n",
            "Epoch 410: Loss = 0.7282\n",
            "Epoch 420: Loss = 0.7215\n",
            "Epoch 430: Loss = 0.7131\n",
            "Epoch 440: Loss = 0.7071\n",
            "Epoch 450: Loss = 0.6945\n",
            "Epoch 460: Loss = 0.6889\n",
            "Epoch 470: Loss = 0.6819\n",
            "Epoch 480: Loss = 0.6728\n",
            "Epoch 490: Loss = 0.6654\n",
            "Epoch 500: Loss = 0.6587\n",
            "Epoch 510: Loss = 0.6524\n",
            "Epoch 520: Loss = 0.6465\n",
            "Epoch 530: Loss = 0.6383\n",
            "Epoch 540: Loss = 0.6281\n",
            "Epoch 550: Loss = 0.6270\n",
            "Epoch 560: Loss = 0.6175\n",
            "Epoch 570: Loss = 0.6085\n",
            "Epoch 580: Loss = 0.6013\n",
            "Epoch 590: Loss = 0.6010\n",
            "Epoch 600: Loss = 0.5916\n",
            "Epoch 610: Loss = 0.5866\n",
            "Epoch 620: Loss = 0.5812\n",
            "Epoch 630: Loss = 0.5781\n",
            "Epoch 640: Loss = 0.5693\n",
            "Epoch 650: Loss = 0.5606\n",
            "Epoch 660: Loss = 0.5556\n",
            "Epoch 670: Loss = 0.5485\n",
            "Epoch 680: Loss = 0.5510\n",
            "Epoch 690: Loss = 0.5396\n",
            "Epoch 700: Loss = 0.5379\n",
            "Epoch 710: Loss = 0.5310\n",
            "Epoch 720: Loss = 0.5260\n",
            "Epoch 730: Loss = 0.5170\n",
            "Epoch 740: Loss = 0.5146\n",
            "Epoch 750: Loss = 0.5082\n",
            "Epoch 760: Loss = 0.5019\n",
            "Epoch 770: Loss = 0.4995\n",
            "Epoch 780: Loss = 0.5028\n",
            "Epoch 790: Loss = 0.4911\n",
            "Epoch 800: Loss = 0.4884\n",
            "Epoch 810: Loss = 0.4879\n",
            "Epoch 820: Loss = 0.4780\n",
            "Epoch 830: Loss = 0.4669\n",
            "Epoch 840: Loss = 0.4641\n",
            "Epoch 850: Loss = 0.4671\n",
            "Epoch 860: Loss = 0.4602\n",
            "Epoch 870: Loss = 0.4515\n",
            "Epoch 880: Loss = 0.4477\n",
            "Epoch 890: Loss = 0.4490\n",
            "Epoch 900: Loss = 0.4449\n",
            "Epoch 910: Loss = 0.4354\n",
            "Epoch 920: Loss = 0.4339\n",
            "Epoch 930: Loss = 0.4314\n",
            "Epoch 940: Loss = 0.4212\n",
            "Epoch 950: Loss = 0.4204\n",
            "Epoch 960: Loss = 0.4137\n",
            "Epoch 970: Loss = 0.4112\n",
            "Epoch 980: Loss = 0.4091\n",
            "Epoch 990: Loss = 0.4002\n"
          ]
        }
      ]
    }
  ]
}