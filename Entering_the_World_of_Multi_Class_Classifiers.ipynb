{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmlUrBM7tecPRqP4gatdU5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sayed-Hossein-Hosseini/A_Journey_into_the_Depths_of_Neural_Networks/blob/master/Entering_the_World_of_Multi_Class_Classifiers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Entering the World of Multi Class Classifiers**"
      ],
      "metadata": {
        "id": "hWHn0Eqs-CnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Libraries**"
      ],
      "metadata": {
        "id": "KguaGt3_-L5O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CAnjJNG52KfX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading Dataset and Normalization**"
      ],
      "metadata": {
        "id": "5E4qHRR1-jET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load in the data\n",
        "    cifar10 = tf.keras.datasets.cifar10\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "    X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "    print(X_train.shape)\n",
        "    print(X_test.shape)\n",
        "\n",
        "    print(y_train.shape)\n",
        "    print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUYvEIMG-jVy",
        "outputId": "ecebd17f-2f62-43b7-d489-6d0aa878cd43"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "(50000, 1)\n",
            "(10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Flatten and Reshape Images**"
      ],
      "metadata": {
        "id": "Jr8zn-yv-wj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # reshape images from (32, 32, 3) to (3072,)\n",
        "    X_train = X_train.reshape((X_train.shape[0], -1))  # (50000, 3072)\n",
        "    X_test = X_test.reshape((X_test.shape[0], -1))     # (10000, 3072)\n",
        "\n",
        "    y_train, y_test = y_train.flatten(), y_test.flatten() # (50000,  -   10000,)\n",
        "    print(y_train[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3ZYECpH-11B",
        "outputId": "173f161b-515d-4780-dff5-9b5c21093472"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6 9 9 4 1 1 2 7 8 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **One-Hot Encoding**"
      ],
      "metadata": {
        "id": "jR-E_GUrBf1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "print(y_train[:10])\n",
        "print(y_test[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n76t9Op8Bhay",
        "outputId": "fc92e9cb-8317-4167-fa6a-42e39f5dcbb3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 10)\n",
            "(10000, 10)\n",
            "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model**"
      ],
      "metadata": {
        "id": "_3vU69YNNEjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Activation Function : Sigmoid and Relu and Softmax**"
      ],
      "metadata": {
        "id": "PqAkZHU2NIEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation:\n",
        "    @staticmethod\n",
        "    def sigmoid(z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(a):\n",
        "        return a * (1 - a)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu_derivative(z):\n",
        "        return (z > 0).astype(float)\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(z):\n",
        "        exps = np.exp(z - np.max(z, axis=1, keepdims=True))  # For numerical stability\n",
        "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax_derivative(output):\n",
        "        # The exact derivative of softmax is a Jacobian matrix.\n",
        "        # However, when used with cross-entropy loss, the gradient simplifies to:\n",
        "        # ∂L/∂z = y_pred - y_true\n",
        "        # So this method is typically not needed explicitly during backpropagation.\n",
        "        pass"
      ],
      "metadata": {
        "id": "eYOELm9CNRKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Neural Network**"
      ],
      "metadata": {
        "id": "F0ig8k7MQj6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dence Layer Class**"
      ],
      "metadata": {
        "id": "yP8UkBT_QnIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer:\n",
        "    def __init__(self, input_size, output_size, activation='sigmoid'):\n",
        "        self.w = np.random.randn(input_size, output_size) * 0.01\n",
        "        self.b = np.zeros((1, output_size))\n",
        "        self.activation_name = activation\n",
        "        self.z = None\n",
        "        self.a = None\n",
        "        self.input = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.input = x\n",
        "        self.z = np.dot(x, self.w) + self.b\n",
        "        if self.activation_name == 'sigmoid':\n",
        "            self.a = Activation.sigmoid(self.z)\n",
        "        elif self.activation_name == 'relu':\n",
        "            self.a = Activation.relu(self.z)\n",
        "        elif self.activation_name == 'softmax':\n",
        "            self.a = Activation.softmax(self.z)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation function: {self.activation_name}\")\n",
        "        return self.a\n",
        "\n",
        "    def backward(self, da, lr):\n",
        "        m = self.input.shape[0]\n",
        "\n",
        "        # Derivative depending on activation function\n",
        "        if self.activation_name == 'sigmoid':\n",
        "            dz = da * Activation.sigmoid_derivative(self.a)\n",
        "        elif self.activation_name == 'relu':\n",
        "            dz = da * Activation.relu_derivative(self.z)\n",
        "        elif self.activation_name == 'softmax':\n",
        "            # With softmax + cross-entropy, da is already (y_pred - y_true)\n",
        "            dz = da\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation function: {self.activation_name}\")\n",
        "\n",
        "        # Compute gradients\n",
        "        dw = np.dot(self.input.T, dz) / m\n",
        "        db = np.sum(dz, axis=0, keepdims=True) / m\n",
        "        da_prev = np.dot(dz, self.w.T)\n",
        "\n",
        "        # Update weights\n",
        "        self.w -= lr * dw\n",
        "        self.b -= lr * db\n",
        "\n",
        "        return da_prev"
      ],
      "metadata": {
        "id": "MK5kgqoXQn_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Neural Network Class**"
      ],
      "metadata": {
        "id": "uUI5mb0kSyx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        epsilon = 1e-8\n",
        "        if y_true.shape[1] == 1:  # Binary classification\n",
        "            return -np.mean(y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))\n",
        "        else:  # Multi-class classification\n",
        "            return -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=1))\n",
        "\n",
        "    def backward(self, y_true, y_pred, lr):\n",
        "        if y_true.shape[1] == 1:  # Binary classification (sigmoid + BCE)\n",
        "            dz = y_pred - y_true\n",
        "        else:  # Multi-class classification (softmax + CCE)\n",
        "            dz = y_pred - y_true  # Gradient for softmax + cross-entropy\n",
        "        gradient = dz\n",
        "\n",
        "        for layer in reversed(self.layers):\n",
        "            gradient = layer.backward(gradient, lr)\n",
        "\n",
        "    def train(self, X, y, epochs=100, lr=0.01, batch_size=64, verbose=True):\n",
        "        for epoch in range(epochs):\n",
        "            # Shuffle data\n",
        "            indices = np.arange(X.shape[0])\n",
        "            np.random.shuffle(indices)\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            epoch_loss = []\n",
        "\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "                x_batch = X_shuffled[i:i+batch_size]\n",
        "                y_batch = y_shuffled[i:i+batch_size]\n",
        "\n",
        "                # Forward pass\n",
        "                output = x_batch\n",
        "                for layer in self.layers:\n",
        "                    output = layer.forward(output)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.compute_loss(y_batch, output)\n",
        "                epoch_loss.append(loss)\n",
        "\n",
        "                # Backward pass\n",
        "                self.backward(y_batch, output, lr)\n",
        "\n",
        "            if verbose and epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}: Loss = {np.mean(epoch_loss):.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = self.forward(X)\n",
        "        if y_pred.shape[1] == 1:  # Binary classification\n",
        "            return (y_pred >= 0.5).astype(int)\n",
        "        else:  # Multi-class classification\n",
        "            return np.argmax(y_pred, axis=1)"
      ],
      "metadata": {
        "id": "m5TPuv8cS3hh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}